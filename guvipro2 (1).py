# -*- coding: utf-8 -*-
"""GuviPro2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WXjv48X01PGcclLU8XZhClyeddGM6iWt
"""

pip install boto3

import requests
import zipfile
import os
import json
import boto3

# To download ZIP file
def download_zip_file(url, filename):
    r = requests.get(url, stream=True, headers={'User-Agent': 'Mozilla/5.0'})
    if r.status_code == 200:
        with open(filename, 'wb') as f:
            r.raw.decode_content = True
            f.write(r.content)
            print('Zip File Downloading Completed')


url = 'https://www.sec.gov/Archives/edgar/daily-index/bulkdata/submissions.zip'
# filename = url.split('/')[-1]
SourceData = "/content/DataMigration/submissions.zip"
download_zip_file(url, SourceData)

# To Unzip file
unzipping_file = '/content/UnzipFile/s3'
TotalFiles = 10

with zipfile.ZipFile(SourceData,'r') as zip_ref:
  for file_info in zip_ref.infolist():
    if file_info.filename.endswith('.json') and TotalFiles>0:
      zip_ref.extract(file_info.filename,unzipping_file)
      TotalFiles -= 1

# here we first upload json file to Amazon S3 bucket and then read json file from S3 and then upload those data into the Dynamo Database
bucketName = "guvidatamig"
access_key = "AKIAZXWJBHC"
secret_key = "Cgsr95vWLu0"
dynamoTableName = 'GuviDataMig'
s3_client = boto3.client("s3",aws_access_key_id = access_key,aws_secret_access_key = secret_key)
dynamo_client = boto3.client('dynamodb',aws_access_key_id = access_key,aws_secret_access_key = secret_key,region_name= 'us-east-1')

responseS3 = s3_client.create_bucket(Bucket=bucketName)
responseDy = s3_client.list_objects_v2(Bucket=bucketName)
my_bucket = s3_client.list_buckets()
UploadData = '/content/UnzipFile/s3'
for root, dirs,files in os.walk(UploadData):
  for file in files:
    local_file_path = os.path.join(root,file)
    s3_key = os.path.relpath(local_file_path,UploadData)

    s3_client.upload_file(local_file_path,bucketName,s3_key)

for obj in responseDy.get('Contents',[]):
  object_key = obj['Key']
  s3_response = s3_client.get_object(Bucket=bucketName,Key=object_key)
  jsonFileContent = s3_response['Body'].read().decode('utf-8')
  print(f'Json Content for {object_key}:')
  print(jsonFileContent)

  try:
    data = json.loads(jsonFileContent)
  except json.JSONDecodeError as e:
    print(f'Error decoding JSON for {object_key}:{e}')
    continue
  attribute_map = {}

  for key,value in data.items():
    attribute_map[key] = {'S': str(value) if isinstance(value,(str,int,float)) else json.dumps(value)}
  try:
    dynamo_client.put_item(TableName = dynamoTableName, Item=attribute_map)
    print(f'Uploaded data from {object_key} to DynamoDB')
  except Exception as e:
    print(f'Error while uploading for {object_key}:{e}')
    continue
print("Task Completed")
